{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Label Studio to COCO Converter (LS2COCO.ipynb) — ARI3129 Assignment 2025/26\n",
        "\n",
        "This notebook provides a structured and reliable method for converting **Label Studio (JSON) annotation exports** into a **COCO-formatted dataset** suitable for model training within the **ARI3129 Object Detection and Classification Assignment (2025/26)**.\n",
        "\n",
        "## Preliminaries \n",
        "\n",
        "### Purpose of this Conversion\n",
        "Label Studio produces annotation files in its own JSON format, which cannot be used directly by object detection frameworks such as **YOLO**, **Detectron2**, or **MMDetection**.  \n",
        "The **COCO (Common Objects in Context)** format, on the other hand, is a well-defined and widely adopted standard for representing object detection datasets.  \n",
        "This notebook bridges the gap by automatically converting and restructuring the exported data into the correct COCO format, ensuring full compatibility with training pipelines.\n",
        "\n",
        "### Functionality Overview\n",
        "The notebook:\n",
        "- Accepts a **Label Studio `.json` export** and the corresponding **`.zip` of annotated images**.  \n",
        "- Validates, cleans, and normalises the annotations to remove inconsistencies or formatting issues.  \n",
        "- Generates the following outputs:  \n",
        "  - **`COCO.json`** – a unified dataset containing all annotations in COCO format.  \n",
        "  - **`COCO_[attribute].json`** – additional datasets automatically derived according to selected attributes such as *view_angle*, *mounting*, *condition*, and *sign_shape*.  \n",
        "- Includes an optional **train/validation/test splitting utility** that produces reproducible dataset partitions with configurable ratios.\n",
        "\n",
        "### When and How to Use\n",
        "Run this notebook **after completing annotation in Label Studio** and **before initiating model training**.  \n",
        "It ensures that all annotations are consistent, properly structured, and formatted in accordance with COCO standards, allowing for seamless integration into YOLO and other object detection frameworks.\n",
        "\n",
        "<div style=\"border:3px solid #c00; background:#fff7f7; padding:12px; border-radius:6px\">\n",
        "    <strong style=\"color:#000000\">⚠️ IMPORTANT</strong>\n",
        "    <ul>\n",
        "        <li style=\"color:#000000\"><strong>Do NOT use \"Run All\".</strong> Execute cells individually in order to avoid re-initialising the UI or overwriting the environment.</li>\n",
        "        <li style=\"color:#000000\">If you see unexpected errors, re-run only the specific setup or UI cell needed — do not re-run all notebook cells.</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Environment and Dependency Setup\n",
        "\n",
        "This section ensures that all required Python packages are available before the notebook is run.  \n",
        "If any dependency such as `ipywidgets` or `IPython` is missing, it is automatically installed using `pip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd4c586",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def ensure_package(pkg: str, import_name: str | None = None):\n",
        "    try:\n",
        "        return importlib.import_module(import_name or pkg)\n",
        "    except ImportError:\n",
        "        print(f\"Installing missing package: {pkg}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "        return importlib.import_module(import_name or pkg)\n",
        "\n",
        "# --- Core dependencies ---\n",
        "ipywidgets = ensure_package(\"ipywidgets\")\n",
        "IPython_display = ensure_package(\"IPython.display\", \"IPython.display\")\n",
        "\n",
        "# --- Standard library ---\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple, Set\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timezone\n",
        "from time import monotonic\n",
        "import hashlib, json, os, shutil, zipfile\n",
        "\n",
        "# --- Imports from installed packages ---\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "print(\"All required packages are installed and imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06fb6ee",
      "metadata": {},
      "source": [
        "## 2) Singleton Guard and Conversion Setup\n",
        "\n",
        "This code snippet makes sure the LS2COCO app only starts **once** per session.  \n",
        "If the app is already running (like after re-running a cell), it reuses the existing version instead of creating a new one.  \n",
        "This avoids problems and keeps the notebook clean.\n",
        "\n",
        "After that, the notebook creates the folders it needs (`Temp`, `Outputs`, and `Outputs/images`) and sets up some helper functions that will be used later during the conversion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b2c173",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If an app instance already exists, reuse it and skip setup.\n",
        "_SKIP_INIT = False\n",
        "if \"__LS2COCO_APP__\" in globals():\n",
        "    app = globals()[\"__LS2COCO_APP__\"]\n",
        "    with app[\"status_output\"]:\n",
        "        clear_output(wait=False)\n",
        "    display(app[\"controls\"])\n",
        "    _SKIP_INIT = True\n",
        "\n",
        "if not _SKIP_INIT:\n",
        "    # ---- CONSTANTS & DIRECTORIES ----\n",
        "    ROOT_DIR   = Path.cwd()\n",
        "    TEMP_DIR   = ROOT_DIR / \"Temp\"          # was: Inputs\n",
        "    OUTPUT_DIR = ROOT_DIR / \"Outputs\"\n",
        "    IMAGES_DIR = OUTPUT_DIR / \"images\"\n",
        "    LOCK_FILE  = OUTPUT_DIR / \".run.lock\"\n",
        "\n",
        "    for d in [TEMP_DIR, OUTPUT_DIR, IMAGES_DIR]:\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "    # ---- HELPERS: geometry, normalisation, COCO conversion ----\n",
        "    def _bbox_pct_to_abs(val: Dict[str, Any], W: float, H: float) -> Tuple[float, float, float, float]:\n",
        "        x = float(val[\"x\"]) * 0.01 * W\n",
        "        y = float(val[\"y\"]) * 0.01 * H\n",
        "        w = float(val[\"width\"]) * 0.01 * W\n",
        "        h = float(val[\"height\"]) * 0.01 * H\n",
        "        return x, y, w, h\n",
        "\n",
        "    def _float_box_key(val: Dict[str, Any]) -> Tuple[int, int, int, int]:\n",
        "        return (round(val.get(\"x\", 0), 4),\n",
        "                round(val.get(\"y\", 0), 4),\n",
        "                round(val.get(\"width\", 0), 4),\n",
        "                round(val.get(\"height\", 0), 4))\n",
        "\n",
        "    def _norm_attr_name(name: str) -> str:\n",
        "        name = (name or \"\").strip()\n",
        "        return \"\".join(ch.lower() if ch.isalnum() else \"_\" for ch in name).strip(\"_\")\n",
        "\n",
        "    def _has_region_geometry(val: Dict[str, Any]) -> bool:\n",
        "        return all(k in val for k in (\"x\", \"y\", \"width\", \"height\"))\n",
        "\n",
        "    def convert_labelstudio_to_coco(ls_tasks: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        images, annotations = [], []\n",
        "        categories_by_name: Dict[str, int] = {}\n",
        "        next_ann_id, next_img_id = 1, 1\n",
        "\n",
        "        # Category discovery\n",
        "        label_names = set()\n",
        "        for task in ls_tasks:\n",
        "            for ann in task.get(\"annotations\", []):\n",
        "                for res in ann.get(\"result\", []):\n",
        "                    if res.get(\"type\") == \"rectanglelabels\":\n",
        "                        labs = (res.get(\"value\", {}) or {}).get(\"rectanglelabels\") or []\n",
        "                        if labs:\n",
        "                            label_names.add(labs[0])\n",
        "        if not label_names:\n",
        "            label_names = {\"object\"}\n",
        "        for i, name in enumerate(sorted(label_names), start=1):\n",
        "            categories_by_name[name] = i\n",
        "\n",
        "        # Build images + annotations\n",
        "        for task in ls_tasks:\n",
        "            file_name = task.get(\"file_upload\") or Path(task.get(\"data\", {}).get(\"image\", \"\")).name\n",
        "\n",
        "            # pick any dims\n",
        "            W = H = None\n",
        "            for ann in task.get(\"annotations\", []):\n",
        "                for res in ann.get(\"result\", []):\n",
        "                    ow, oh = res.get(\"original_width\"), res.get(\"original_height\")\n",
        "                    if ow and oh:\n",
        "                        W, H = float(ow), float(oh)\n",
        "                        break\n",
        "                if W and H:\n",
        "                    break\n",
        "            if not (W and H):\n",
        "                continue\n",
        "\n",
        "            image_id = next_img_id\n",
        "            next_img_id += 1\n",
        "            img_obj: Dict[str, Any] = {\"id\": image_id, \"file_name\": file_name, \"width\": int(W), \"height\": int(H)}\n",
        "            images.append(img_obj)\n",
        "\n",
        "            # Prepare holders\n",
        "            attrs_by_region_id: Dict[str, Dict[str, Any]] = {}\n",
        "            attrs_by_box_key: Dict[Tuple[int, int, int, int], Dict[str, Any]] = {}\n",
        "            image_level_attrs: Dict[str, Any] = {}\n",
        "\n",
        "            # Collect non-rectangle results\n",
        "            for ann in task.get(\"annotations\", []):\n",
        "                for res in ann.get(\"result\", []):\n",
        "                    rtype = res.get(\"type\")\n",
        "                    if rtype == \"rectanglelabels\":\n",
        "                        continue\n",
        "                    val = res.get(\"value\", {}) or {}\n",
        "                    if not isinstance(val, dict):\n",
        "                        continue\n",
        "                    from_name = _norm_attr_name(res.get(\"from_name\") or \"\")\n",
        "                    if not from_name:\n",
        "                        continue\n",
        "\n",
        "                    # payload extraction\n",
        "                    if rtype in (\"choices\", \"labels\", \"select\"):\n",
        "                        payload = val.get(\"choices\") or val.get(\"labels\") or []\n",
        "                    elif rtype in (\"textarea\", \"text\"):\n",
        "                        t = val.get(\"text\")\n",
        "                        payload = t if isinstance(t, list) else (t or val.get(\"value\"))\n",
        "                    elif rtype == \"number\":\n",
        "                        payload = val.get(\"number\")\n",
        "                    elif rtype == \"rating\":\n",
        "                        payload = val.get(\"rating\")\n",
        "                    else:\n",
        "                        payload = val  # fallback\n",
        "\n",
        "                    # Decide: region-linked or GLOBAL\n",
        "                    linked_region_id = res.get(\"from_id\")\n",
        "                    if linked_region_id or _has_region_geometry(val):\n",
        "                        key = _float_box_key(val)\n",
        "                        if linked_region_id:\n",
        "                            attrs_by_region_id.setdefault(linked_region_id, {})\n",
        "                            attrs_by_region_id[linked_region_id][from_name] = payload\n",
        "                        if any(key):\n",
        "                            attrs_by_box_key.setdefault(key, {})\n",
        "                            attrs_by_box_key[key][from_name] = payload\n",
        "                    else:\n",
        "                        # GLOBAL (image-level) attribute\n",
        "                        if isinstance(payload, list):\n",
        "                            existing = image_level_attrs.get(from_name, [])\n",
        "                            if not isinstance(existing, list):\n",
        "                                existing = [existing] if existing is not None else []\n",
        "                            image_level_attrs[from_name] = list(dict.fromkeys(existing + payload))\n",
        "                        else:\n",
        "                            image_level_attrs[from_name] = payload\n",
        "\n",
        "            if image_level_attrs:\n",
        "                img_obj[\"attributes\"] = image_level_attrs\n",
        "\n",
        "            # Emit rectangles with attributes\n",
        "            for ann in task.get(\"annotations\", []):\n",
        "                for res in ann.get(\"result\", []):\n",
        "                    if res.get(\"type\") != \"rectanglelabels\":\n",
        "                        continue\n",
        "                    rect_id = res.get(\"id\")\n",
        "                    val = res.get(\"value\", {}) or {}\n",
        "                    labs = val.get(\"rectanglelabels\") or [\"object\"]\n",
        "                    label = labs[0]\n",
        "                    cat_id = categories_by_name[label]\n",
        "                    x, y, w, h = _bbox_pct_to_abs(val, W, H)\n",
        "\n",
        "                    merged: Dict[str, Any] = {}\n",
        "                    if rect_id and rect_id in attrs_by_region_id:\n",
        "                        merged.update(attrs_by_region_id[rect_id])\n",
        "                    else:\n",
        "                        k = _float_box_key(val)\n",
        "                        if k in attrs_by_box_key:\n",
        "                            merged.update(attrs_by_box_key[k])\n",
        "\n",
        "                    alias_map = {\n",
        "                        \"view_angle\": {\"view_angle\", \"viewangle\", \"view\", \"view_angles\"},\n",
        "                        \"mounting\": {\"mounting\", \"mount\"},\n",
        "                        \"condition\": {\"condition\", \"state\"},\n",
        "                        \"sign_shape\": {\"sign_shape\", \"shape\", \"signshape\"},\n",
        "                    }\n",
        "                    surfaced = {}\n",
        "                    for key, aliases in alias_map.items():\n",
        "                        for a in aliases:\n",
        "                            if a in merged:\n",
        "                                surfaced[key] = merged[a]\n",
        "                                break\n",
        "\n",
        "                    attrs_out = dict(merged)\n",
        "                    for k in (\"view_angle\", \"mounting\", \"condition\", \"sign_shape\"):\n",
        "                        if k not in attrs_out:\n",
        "                            attrs_out[k] = []\n",
        "                    attrs_out.update(surfaced)\n",
        "\n",
        "                    ann_obj = {\n",
        "                        \"id\": next_ann_id,\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": cat_id,\n",
        "                        \"bbox\": [x, y, w, h],\n",
        "                        \"iscrowd\": 0,\n",
        "                        \"area\": float(w) * float(h),\n",
        "                        \"attributes\": attrs_out,\n",
        "                    }\n",
        "                    annotations.append(ann_obj)\n",
        "                    next_ann_id += 1\n",
        "\n",
        "        coco = {\n",
        "            \"info\": {\n",
        "                \"description\": \"LS→COCO with region + global attributes (view_angle, mounting, condition, sign_shape surfaced on annotations)\",\n",
        "                \"date_created\": datetime.now(timezone.utc).isoformat(),\n",
        "            },\n",
        "            \"licenses\": [],\n",
        "            \"images\": images,\n",
        "            \"annotations\": annotations,\n",
        "            \"categories\": [{\"id\": cid, \"name\": name} for name, cid in categories_by_name.items()],\n",
        "        }\n",
        "        return coco\n",
        "\n",
        "    def _ensure_coco_annotation(src_path: Path, out_path: Path) -> Path:\n",
        "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Already COCO\n",
        "        if isinstance(data, dict) and {\"images\", \"annotations\", \"categories\"}.issubset(data.keys()):\n",
        "            changed = False\n",
        "            for ann in data.get(\"annotations\", []):\n",
        "                attrs = ann.setdefault(\"attributes\", {})\n",
        "                for k in (\"view_angle\", \"mounting\", \"condition\", \"sign_shape\"):\n",
        "                    if k not in attrs:\n",
        "                        attrs[k] = []\n",
        "                        changed = True\n",
        "            if changed:\n",
        "                with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(data, f, indent=2)\n",
        "                return out_path\n",
        "            return src_path\n",
        "\n",
        "        # LS array → COCO\n",
        "        if isinstance(data, list):\n",
        "            coco = convert_labelstudio_to_coco(data)\n",
        "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(coco, f, indent=2)\n",
        "            print(f\"Converted Label Studio JSON → COCO at {out_path}\")\n",
        "            return out_path\n",
        "\n",
        "        raise ValueError(f\"{src_path} is neither valid COCO nor LS array.\")\n",
        "\n",
        "    # ---- REMAP HELPERS ----\n",
        "    def _build_image_index(images_root: Path):\n",
        "        by_base = defaultdict(list)\n",
        "        by_base_lower = defaultdict(list)\n",
        "        by_stem_lower = defaultdict(list)\n",
        "        for p in images_root.rglob(\"*\"):\n",
        "            if p.is_file() and not p.name.startswith(\"._\") and p.suffix.lower() in IMAGE_EXTENSIONS:\n",
        "                rel = p.relative_to(images_root).as_posix()\n",
        "                base = p.name\n",
        "                stem = p.stem\n",
        "                by_base[base].append(rel)\n",
        "                by_base_lower[base.lower()].append(rel)\n",
        "                by_stem_lower[stem.lower()].append(rel)\n",
        "        return by_base, by_base_lower, by_stem_lower\n",
        "\n",
        "    def _pick_one(candidates):\n",
        "        return sorted(candidates, key=lambda s: (len(s), s))[0]\n",
        "\n",
        "    def remap_coco_file_names(coco_json_path: Path, images_root: Path):\n",
        "        with open(coco_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            coco = json.load(f)\n",
        "        by_base, by_base_lower, by_stem_lower = _build_image_index(images_root)\n",
        "\n",
        "        missing, fixed = [], 0\n",
        "        for im in coco.get(\"images\", []):\n",
        "            fname = im.get(\"file_name\", \"\")\n",
        "            if not fname:\n",
        "                missing.append(\"(empty)\")\n",
        "                continue\n",
        "\n",
        "            candidate = images_root / fname\n",
        "            if candidate.exists():\n",
        "                continue\n",
        "\n",
        "            base = Path(fname).name\n",
        "            stem = Path(fname).stem\n",
        "            lower_base, lower_stem = base.lower(), stem.lower()\n",
        "            new_rel = None\n",
        "\n",
        "            if base in by_base:\n",
        "                new_rel = _pick_one(by_base[base])\n",
        "            elif lower_base in by_base_lower:\n",
        "                new_rel = _pick_one(by_base_lower[lower_base])\n",
        "            elif lower_stem in by_stem_lower:\n",
        "                new_rel = _pick_one(by_stem_lower[lower_stem])\n",
        "            else:\n",
        "                prefix = stem.split(\"-\")[0].lower()\n",
        "                if prefix and len(prefix) >= 4:\n",
        "                    prefixed = []\n",
        "                    for key_stem, paths in by_stem_lower.items():\n",
        "                        if key_stem.startswith(prefix):\n",
        "                            prefixed.extend(paths)\n",
        "                    if prefixed:\n",
        "                        new_rel = _pick_one(prefixed)\n",
        "\n",
        "            if new_rel:\n",
        "                im[\"file_name\"] = new_rel\n",
        "                fixed += 1\n",
        "            else:\n",
        "                missing.append(fname)\n",
        "\n",
        "        with open(coco_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(coco, f, indent=2)\n",
        "\n",
        "        print(f\"Remapped {fixed} / {len(coco.get('images', []))} image paths. Missing/Duplicate: {len(missing)}\")\n",
        "        if missing:\n",
        "            print('Missing/Duplicate examples:', ', '.join(missing[:10]), '...')\n",
        "        return coco_json_path\n",
        "\n",
        "    # Stash the environment so the next cell can build the UI.\n",
        "    globals()[\"__LS2COCO_ENV__\"] = {\n",
        "        \"ROOT_DIR\": ROOT_DIR,\n",
        "        \"TEMP_DIR\": TEMP_DIR,               \n",
        "        \"OUTPUT_DIR\": OUTPUT_DIR,\n",
        "        \"IMAGES_DIR\": IMAGES_DIR,\n",
        "        \"LOCK_FILE\": LOCK_FILE,\n",
        "        \"IMAGE_EXTENSIONS\": IMAGE_EXTENSIONS,\n",
        "        \"helpers\": {\n",
        "            \"_ensure_coco_annotation\": _ensure_coco_annotation,\n",
        "            \"remap_coco_file_names\": remap_coco_file_names,\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85074804",
      "metadata": {},
      "source": [
        "## 3) Collect Dataset inputs & Conversion to COCO\n",
        "\n",
        "### What You Need to Upload\n",
        "\n",
        "1. **Label Studio JSON file (`.json`)**  \n",
        "   - This should be the *exported annotations file* containing all labelled tasks from Label Studio.  \n",
        "   - Make sure it includes the annotations from all team members before uploading.  \n",
        "\n",
        "2. **Images ZIP file (`.zip`)**  \n",
        "   - This must contain *all the images used in annotation*.  \n",
        "   - Combine all image folders into one archive before uploading.  \n",
        "   - The file names in the ZIP must correspond exactly to those referenced in the Label Studio JSON file.\n",
        "\n",
        "Once both files are uploaded:\n",
        "- Press **“Run Conversion”** to start the process.  \n",
        "- The notebook will automatically extract and validate the images, convert the JSON into COCO format, and generate a `COCO.json` file inside the `Outputs` directory.\n",
        "\n",
        "### Output Files\n",
        "\n",
        "-  **`COCO.json`** — Final, cleaned dataset in COCO format.  \n",
        "- NB: A `.bak` file is automatically created when missing images are detected, allowing you to review the original data if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9fb027",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If an app already exists, show it and stop.\n",
        "if \"__LS2COCO_APP__\" in globals():\n",
        "    app = globals()[\"__LS2COCO_APP__\"]\n",
        "    with app[\"status_output\"]:\n",
        "        clear_output(wait=False)\n",
        "    display(app[\"controls\"])\n",
        "else:\n",
        "    # Ensure setup cell has been run\n",
        "    if \"__LS2COCO_ENV__\" not in globals():\n",
        "        raise RuntimeError(\"Please run the setup cell (1) before this UI cell (2).\")\n",
        "\n",
        "    # Pull environment\n",
        "    ENV = globals()[\"__LS2COCO_ENV__\"]\n",
        "    ROOT_DIR   = ENV[\"ROOT_DIR\"]\n",
        "    TEMP_DIR   = ENV[\"TEMP_DIR\"]          # was: INPUT_DIR\n",
        "    OUTPUT_DIR = ENV[\"OUTPUT_DIR\"]\n",
        "    IMAGES_DIR = ENV[\"IMAGES_DIR\"]\n",
        "    LOCK_FILE  = ENV[\"LOCK_FILE\"]\n",
        "    IMAGE_EXTENSIONS = ENV[\"IMAGE_EXTENSIONS\"]\n",
        "    _ensure_coco_annotation = ENV[\"helpers\"][\"_ensure_coco_annotation\"]\n",
        "    remap_coco_file_names   = ENV[\"helpers\"][\"remap_coco_file_names\"]\n",
        "\n",
        "    # Widgets\n",
        "    json_uploader = widgets.FileUpload(accept=\".json\", multiple=False, description=\"Upload JSON\")\n",
        "    zip_uploader  = widgets.FileUpload(accept=\".zip\",  multiple=False, description=\"Upload ZIP\")\n",
        "    run_button    = widgets.Button(description=\"Run Conversion\", button_style=\"success\", icon=\"check\")\n",
        "    status_output = widgets.Output(layout=widgets.Layout(border=\"1px solid #ddd\", padding=\"8px\"))\n",
        "\n",
        "    controls = widgets.VBox([\n",
        "        widgets.HTML(\"<b>1)</b> Upload the consolidated Label Studio JSON annotation file (include all team members).\"),\n",
        "        json_uploader,\n",
        "        widgets.HTML(\"<b>2)</b> Upload a ZIP containing all images referenced by the JSON (merged into one archive).\"),\n",
        "        zip_uploader,\n",
        "        run_button,\n",
        "        status_output,\n",
        "    ])\n",
        "    display(controls)\n",
        "\n",
        "    # --- Handler utilities ---\n",
        "    def _purge_all_click_handlers(btn: widgets.Button):\n",
        "        try:\n",
        "            for cb in list(getattr(btn._click_handlers, \"callbacks\", [])):\n",
        "                try:\n",
        "                    btn.on_click(cb, remove=True)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            btn._click_handlers.callbacks = []\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    _purge_all_click_handlers(run_button)\n",
        "\n",
        "    _last_click_ts = {\"t\": 0.0}\n",
        "    _busy          = {\"flag\": False}\n",
        "    LAST_RUN_SIG   = {\"v\": None}\n",
        "\n",
        "    def _acquire_file_lock(lock_path: Path) -> bool:\n",
        "        try:\n",
        "            lock_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if lock_path.exists():\n",
        "                return False\n",
        "            lock_path.write_text(str(datetime.now()))\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _release_file_lock(lock_path: Path):\n",
        "        try:\n",
        "            if lock_path.exists():\n",
        "                lock_path.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def _as_bytes(x):\n",
        "        if isinstance(x, (bytes, bytearray)):\n",
        "            return bytes(x)\n",
        "        if isinstance(x, memoryview):\n",
        "            return x.tobytes()\n",
        "        try:\n",
        "            return bytes(x)\n",
        "        except Exception:\n",
        "            raise TypeError(f\"Unsupported content type for bytes: {type(x)}\")\n",
        "\n",
        "    def _get_upload_bytes(uploader: widgets.FileUpload) -> bytes:\n",
        "        v = uploader.value\n",
        "        if not v:\n",
        "            raise RuntimeError(\"No file uploaded.\")\n",
        "        if isinstance(v, dict):          # ipywidgets 7\n",
        "            first = next(iter(v.values()))\n",
        "        elif isinstance(v, (tuple, list)):  # ipywidgets 8\n",
        "            first = v[0]\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unexpected FileUpload.value type: {type(v)}\")\n",
        "        return _as_bytes(first[\"content\"])\n",
        "\n",
        "    # --- NEW: prune COCO to only include images that physically exist in IMAGES_DIR ---\n",
        "    def _prune_missing_images(coco_path: Path, images_dir: Path) -> tuple[int, int, int]:\n",
        "        \"\"\"\n",
        "        Returns (kept_images, kept_annotations, removed_images)\n",
        "        \"\"\"\n",
        "        with coco_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Build a case-insensitive set of existing basenames\n",
        "        existing = {p.name.lower() for p in images_dir.rglob(\"*\") if p.is_file()}\n",
        "\n",
        "        # Keep only images whose basename exists\n",
        "        orig_images = data.get(\"images\", [])\n",
        "        keep_images = []\n",
        "        kept_ids = set()\n",
        "        removed = 0\n",
        "        for img in orig_images:\n",
        "            fname = Path(img.get(\"file_name\", \"\")).name.lower()\n",
        "            if fname in existing:\n",
        "                keep_images.append(img)\n",
        "                kept_ids.add(img.get(\"id\"))\n",
        "            else:\n",
        "                removed += 1\n",
        "\n",
        "        # Drop annotations whose image_id was removed\n",
        "        orig_anns = data.get(\"annotations\", [])\n",
        "        keep_anns = [a for a in orig_anns if a.get(\"image_id\") in kept_ids]\n",
        "\n",
        "        # Write back (make backup first)\n",
        "        backup = coco_path.with_suffix(\".json.bak\")\n",
        "        try:\n",
        "            if backup.exists():\n",
        "                backup.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "        shutil.copy2(coco_path, backup)\n",
        "\n",
        "        data[\"images\"] = keep_images\n",
        "        data[\"annotations\"] = keep_anns\n",
        "        with coco_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return len(keep_images), len(keep_anns), removed\n",
        "\n",
        "    # --- Click handler ---\n",
        "    def _handle_run(_):\n",
        "        # Debounce: ignore clicks within 1.0s\n",
        "        now = monotonic()\n",
        "        if now - _last_click_ts[\"t\"] < 1.0:\n",
        "            return\n",
        "        _last_click_ts[\"t\"] = now\n",
        "\n",
        "        # Busy gate\n",
        "        if _busy[\"flag\"]:\n",
        "            return\n",
        "        _busy[\"flag\"] = True\n",
        "\n",
        "        # File lock\n",
        "        got_lock = _acquire_file_lock(LOCK_FILE)\n",
        "        if not got_lock:\n",
        "            _busy[\"flag\"] = False\n",
        "            return\n",
        "\n",
        "        run_button.disabled = True\n",
        "        status_output.clear_output(wait=True)\n",
        "        with status_output:\n",
        "            try:\n",
        "                if not json_uploader.value:\n",
        "                    raise RuntimeError(\"Please upload the annotation JSON.\")\n",
        "                if not zip_uploader.value:\n",
        "                    raise RuntimeError(\"Please upload the images ZIP.\")\n",
        "\n",
        "                # Content-hash: suppress duplicate work for same inputs\n",
        "                json_bytes = _get_upload_bytes(json_uploader)\n",
        "                zip_bytes  = _get_upload_bytes(zip_uploader)\n",
        "                run_sig = hashlib.sha256(json_bytes + b\"|\" + zip_bytes).hexdigest()\n",
        "                if LAST_RUN_SIG[\"v\"] == run_sig:\n",
        "                    display(Markdown(\"**Inputs match the last run; no changes made.**\"))\n",
        "                    return\n",
        "                LAST_RUN_SIG[\"v\"] = run_sig\n",
        "\n",
        "                # Clean temp/output folders\n",
        "                for d in [TEMP_DIR, OUTPUT_DIR]:\n",
        "                    if d.exists():\n",
        "                        shutil.rmtree(d)\n",
        "                    d.mkdir(parents=True, exist_ok=True)\n",
        "                IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # Save uploads into Temp\n",
        "                ann_in_path = TEMP_DIR / \"input.json\"\n",
        "                zip_path    = TEMP_DIR / \"images.zip\"\n",
        "                with open(ann_in_path, \"wb\") as f:\n",
        "                    f.write(json_bytes)\n",
        "                with open(zip_path, \"wb\") as f:\n",
        "                    f.write(zip_bytes)\n",
        "\n",
        "                # Extract and flatten images\n",
        "                temp_extract = IMAGES_DIR.parent / \"_temp_extract\"\n",
        "                if temp_extract.exists():\n",
        "                    shutil.rmtree(temp_extract)\n",
        "                temp_extract.mkdir(parents=True, exist_ok=True)\n",
        "                with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                    zf.extractall(temp_extract)\n",
        "\n",
        "                IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "                for p in temp_extract.rglob(\"*\"):\n",
        "                    if p.is_file():\n",
        "                        if p.name.startswith(\"._\") or \"DS_Store\" in p.name:\n",
        "                            try: p.unlink()\n",
        "                            except Exception: pass\n",
        "                            continue\n",
        "                        if p.suffix.lower() in IMAGE_EXTENSIONS:\n",
        "                            target = IMAGES_DIR / p.name\n",
        "                            if not target.exists():\n",
        "                                shutil.move(str(p), str(target))\n",
        "                shutil.rmtree(temp_extract, ignore_errors=True)\n",
        "\n",
        "                # Count images\n",
        "                n_imgs = sum(1 for p in IMAGES_DIR.rglob(\"*\")\n",
        "                             if p.suffix.lower() in IMAGE_EXTENSIONS and not p.name.startswith(\"._\"))\n",
        "                if n_imgs == 0:\n",
        "                    raise ValueError(f\"No valid images found after extracting ZIP into {IMAGES_DIR}\")\n",
        "\n",
        "                # Convert / ensure COCO\n",
        "                coco_out_path = OUTPUT_DIR / \"COCO.json\"\n",
        "                coco_path = _ensure_coco_annotation(ann_in_path, coco_out_path)\n",
        "\n",
        "                # Remap names to actual files\n",
        "                coco_path = remap_coco_file_names(coco_path, IMAGES_DIR)\n",
        "\n",
        "                # NEW: prune any images/annotations whose files are missing\n",
        "                kept_imgs, kept_anns, removed_imgs = _prune_missing_images(coco_path, IMAGES_DIR)\n",
        "\n",
        "                display(Markdown(\"**Your annotations are ready.**\"))\n",
        "                print(f\"Temp:     {TEMP_DIR} (will be deleted after this run)\")\n",
        "                print(f\"Outputs:  {OUTPUT_DIR}\")\n",
        "                print(f\"COCO JSON:{coco_path}\")\n",
        "                print(f\"Images:   {IMAGES_DIR} ({n_imgs} files)\")\n",
        "                print(f\"Pruned:   removed {removed_imgs} missing/duplicate image entries; kept {kept_imgs} images and {kept_anns} annotations.\")\n",
        "            except Exception as exc:\n",
        "                display(Markdown(f\"**Error:** {exc}\"))\n",
        "            finally:\n",
        "                # Always clean up Temp and release the lock\n",
        "                shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
        "                _release_file_lock(LOCK_FILE)\n",
        "                _busy[\"flag\"] = False\n",
        "                run_button.disabled = False\n",
        "\n",
        "    # Bind once and register app\n",
        "    run_button.on_click(_handle_run)\n",
        "    globals()[\"__LS2COCO_APP__\"] = {\n",
        "        \"controls\": controls,\n",
        "        \"run_button\": run_button,\n",
        "        \"status_output\": status_output,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c697d3",
      "metadata": {},
      "source": [
        "## 4) Attribute-Specific COCO File Generator\n",
        "\n",
        "This section enables the creation of **attribute-specific COCO datasets** derived from the main `COCO.json` file produced in the previous step.  \n",
        "It provides a graphical interface where you can select a particular attribute (*view_angle*, *mounting*, *condition*, or *sign_shape*) and automatically generate new COCO files that contain only the corresponding attribute values.\n",
        "\n",
        "### Purpose of this Step\n",
        "In the ARI3129 assignment, you need to train and evaluate models based on specific object attributes rather than the overall dataset.  \n",
        "For instance, you might wish to train one model for different *sign shapes* and another for *view angles*.  \n",
        "This tool simplifies that process by extracting attribute-focused subsets of your dataset while maintaining full COCO compatibility.\n",
        "\n",
        "After running this step, one or more files will be created inside your `Outputs` directory: **`COCO_<attribute>.json`** — each containing annotations filtered by that specific attribute.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ff5c08",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reuse paths from earlier section\n",
        "DATASET_DIR = OUTPUT_DIR\n",
        "INPUT_JSON_PATH = OUTPUT_DIR / \"COCO.json\"\n",
        "\n",
        "# ---- Constants ----\n",
        "PLACEHOLDER = \"Click to select attribute\"\n",
        "ALL_LABEL   = \"All (Create JSONs for all attributes)\"\n",
        "ALL_KEYS    = [\"view_angle\", \"mounting\", \"condition\", \"sign_shape\"]\n",
        "\n",
        "# ---- IO helpers ----\n",
        "def load_coco(p: Path) -> Dict[str, Any]:\n",
        "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def save_coco(obj: Dict[str, Any], p: Path) -> None:\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with p.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "# ---- Scan attributes present on annotations ----\n",
        "def scan_annotation_attributes(coco: Dict[str, Any]) -> Tuple[Set[str], Dict[str, Set[str]]]:\n",
        "    keys: Set[str] = set()\n",
        "    values_map: Dict[str, Set[str]] = {}\n",
        "    for ann in coco.get(\"annotations\", []):\n",
        "        attrs = ann.get(\"attributes\", {}) or {}\n",
        "        for k, v in attrs.items():\n",
        "            keys.add(k)\n",
        "            if isinstance(v, list):\n",
        "                vals = [str(x) for x in v if str(x).strip()]\n",
        "            elif v is None:\n",
        "                vals = []\n",
        "            else:\n",
        "                vals = [str(v)] if str(v).strip() else []\n",
        "            if vals:\n",
        "                values_map.setdefault(k, set()).update(vals)\n",
        "    return keys, values_map\n",
        "\n",
        "# ---- Core builder (strict: no missing values allowed) ----\n",
        "def build_attribute_coco_strict(full_coco: Dict[str, Any], attribute_key: str) -> Dict[str, Any]:\n",
        "    images = full_coco.get(\"images\", [])\n",
        "    anns   = full_coco.get(\"annotations\", [])\n",
        "    img_by_id = {im[\"id\"]: im for im in images}\n",
        "\n",
        "    # Validate: collect image names for any annotation missing/empty attribute\n",
        "    missing_img_names: List[str] = []\n",
        "    def _vals_of(v):\n",
        "        if isinstance(v, list):\n",
        "            return [str(x) for x in v if str(x).strip()]\n",
        "        elif v is None:\n",
        "            return []\n",
        "        else:\n",
        "            s = str(v).strip()\n",
        "            return [s] if s else []\n",
        "\n",
        "    for ann in anns:\n",
        "        attrs = ann.get(\"attributes\", {}) or {}\n",
        "        vals = _vals_of(attrs.get(attribute_key, []))\n",
        "        if not vals:\n",
        "            im = img_by_id.get(ann[\"image_id\"], {})\n",
        "            fname = im.get(\"file_name\", f\"<image_id:{ann['image_id']}>\")\n",
        "            missing_img_names.append(fname)\n",
        "\n",
        "    if missing_img_names:\n",
        "        missing_img_names = sorted(set(missing_img_names))\n",
        "        raise ValueError(\n",
        "            \"Missing required attribute values for '{}'.\\nThe following image(s) contain \"\n",
        "            \"at least one annotation with an empty/missing value:\\n- \".format(attribute_key)\n",
        "            + \"\\n- \".join(missing_img_names)\n",
        "        )\n",
        "\n",
        "    # Collect unique values\n",
        "    value_set: List[str] = []\n",
        "    value_to_id: Dict[str, int] = {}\n",
        "    def _iter_vals():\n",
        "        for ann in anns:\n",
        "            vals = _vals_of(ann.get(\"attributes\", {}).get(attribute_key, []))\n",
        "            for v in vals:\n",
        "                yield v\n",
        "\n",
        "    for v in _iter_vals():\n",
        "        if v not in value_to_id:\n",
        "            value_to_id[v] = 0\n",
        "            value_set.append(v)\n",
        "\n",
        "    value_set_sorted = sorted(value_set)\n",
        "    value_to_id = {v: i+1 for i, v in enumerate(value_set_sorted)}\n",
        "    categories = [{\"id\": cid, \"name\": v} for v, cid in value_to_id.items()]\n",
        "\n",
        "    # Build new annotations (explode multi)\n",
        "    new_annotations: List[Dict[str, Any]] = []\n",
        "    next_ann_id = 1\n",
        "    for ann in anns:\n",
        "        vals = [str(x) for x in (ann.get(\"attributes\", {}).get(attribute_key, []) if isinstance(ann.get(\"attributes\", {}).get(attribute_key, []), list)\n",
        "                                 else [ann.get(\"attributes\", {}).get(attribute_key, [])]) if str(x).strip()]\n",
        "        for v in vals:\n",
        "            new_annotations.append({\n",
        "                \"id\": next_ann_id,\n",
        "                \"image_id\": ann[\"image_id\"],\n",
        "                \"category_id\": value_to_id[v],\n",
        "                \"bbox\": ann[\"bbox\"],\n",
        "                \"iscrowd\": ann.get(\"iscrowd\", 0),\n",
        "                \"area\": ann.get(\"area\", float(ann[\"bbox\"][2]) * float(ann[\"bbox\"][3])),\n",
        "                \"attributes\": ann.get(\"attributes\", {}),\n",
        "            })\n",
        "            next_ann_id += 1\n",
        "\n",
        "    out = {\n",
        "        \"info\": {\"description\": f\"Attribute COCO for '{attribute_key}'\"},\n",
        "        \"licenses\": full_coco.get(\"licenses\", []),\n",
        "        \"images\": images,\n",
        "        \"annotations\": new_annotations,\n",
        "        \"categories\": categories,\n",
        "    }\n",
        "    return out\n",
        "\n",
        "# ---- UI (placeholder default + 'All' option) ----\n",
        "attr_dropdown = widgets.Dropdown(\n",
        "    options=[PLACEHOLDER], \n",
        "    value=PLACEHOLDER,\n",
        "    description=\"Attribute:\",\n",
        "    layout=widgets.Layout(width=\"400px\")\n",
        ")\n",
        "make_button   = widgets.Button(description=\"Create Attribute COCO\", button_style=\"info\", icon=\"cogs\")\n",
        "output_area   = widgets.Output(layout=widgets.Layout(border=\"1px solid #ddd\", padding=\"8px\"))\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Create attribute-specific COCO Files</b>\"),\n",
        "    widgets.HBox([attr_dropdown]),\n",
        "    make_button,\n",
        "    output_area\n",
        "])\n",
        "\n",
        "def _refresh_attribute_choices():\n",
        "    output_area.clear_output()\n",
        "    if not INPUT_JSON_PATH.exists():\n",
        "        with output_area:\n",
        "            display(Markdown(\"**Outputs/COCO.json not found.** Run the conversion step first.\"))\n",
        "        attr_dropdown.options = [PLACEHOLDER]\n",
        "        attr_dropdown.value = PLACEHOLDER\n",
        "        return\n",
        "\n",
        "    coco = load_coco(INPUT_JSON_PATH)\n",
        "    keys, values_map = scan_annotation_attributes(coco)\n",
        "    viable_keys = sorted([k for k in keys if len(values_map.get(k, [])) > 0])\n",
        "\n",
        "    # Assemble options: placeholder + ALL + viable keys\n",
        "    options = [PLACEHOLDER, ALL_LABEL] + viable_keys\n",
        "    attr_dropdown.options = options\n",
        "    attr_dropdown.value = PLACEHOLDER\n",
        "\n",
        "    with output_area:\n",
        "        display(Markdown(f\"Found {len(viable_keys)} candidate attribute key(s).\"))\n",
        "        for k in viable_keys[:12]:\n",
        "            vals = sorted(list(values_map.get(k, [])))\n",
        "            sample = \", \".join(vals[:6]) + (\" ...\" if len(vals) > 6 else \"\")\n",
        "            print(f\"- {k}: {len(vals)} values [{sample}]\")\n",
        "        if len(viable_keys) > 12:\n",
        "            print(f\"... and {len(viable_keys)-12} more.\")\n",
        "\n",
        "def _make_attribute_coco(_):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        if not INPUT_JSON_PATH.exists():\n",
        "            display(Markdown(\"**COCO.json not found.** Run the previous section first.\"))\n",
        "            return\n",
        "\n",
        "        sel = attr_dropdown.value\n",
        "        if sel == PLACEHOLDER:\n",
        "            display(Markdown(\"**Select an attribute from the dropdown first.**\"))\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            coco = load_coco(INPUT_JSON_PATH)\n",
        "\n",
        "            if sel == ALL_LABEL:\n",
        "                # Run for all four canonical keys; error if any is missing/empty anywhere\n",
        "                created = []\n",
        "                failures = []\n",
        "                for key in ALL_KEYS:\n",
        "                    try:\n",
        "                        derived = build_attribute_coco_strict(coco, attribute_key=key)\n",
        "                        safe_key = \"\".join(ch if ch.isalnum() or ch in (\"_\", \"-\") else \"_\" for ch in key)\n",
        "                        out_path = DATASET_DIR / f\"COCO_{safe_key}.json\"\n",
        "                        save_coco(derived, out_path)\n",
        "                        created.append((key, out_path, len(derived[\"annotations\"]), len(derived[\"categories\"])))\n",
        "                    except Exception as e:\n",
        "                        failures.append((key, str(e)))\n",
        "\n",
        "                if created:\n",
        "                    display(Markdown(\"**Created attribute COCO files:**\"))\n",
        "                    for key, path, nann, ncat in created:\n",
        "                        print(f\"- {key}: {path}  | annotations={nann}  categories={ncat}\")\n",
        "\n",
        "                if failures:\n",
        "                    display(Markdown(\"**Some attributes failed validation:**\"))\n",
        "                    for key, msg in failures:\n",
        "                        print(f\"\\n[{key}]\")\n",
        "                        print(msg)\n",
        "                return\n",
        "\n",
        "            # Single selected attribute\n",
        "            derived = build_attribute_coco_strict(coco, attribute_key=sel)\n",
        "            safe_key = \"\".join(ch if ch.isalnum() or ch in (\"_\", \"-\") else \"_\" for ch in sel)\n",
        "            out_path = DATASET_DIR / f\"COCO_{safe_key}.json\"\n",
        "            save_coco(derived, out_path)\n",
        "\n",
        "            display(Markdown(f\"**Attribute '{sel}' COCO file created.**\"))\n",
        "            print(f\"Saved to: {out_path}\")\n",
        "            print(f\"Images: {len(derived['images'])}\")\n",
        "            print(f\"Annotations: {len(derived['annotations'])}\")\n",
        "            print(f\"Categories ({len(derived['categories'])}):\")\n",
        "            for c in derived[\"categories\"]:\n",
        "                print(f\"  {c['id']}: {c['name']}\")\n",
        "\n",
        "        except ValueError as ve:\n",
        "            display(Markdown(f\"**Validation failed:**\\n\\n```\\n{ve}\\n```\"))\n",
        "        except Exception as exc:\n",
        "            display(Markdown(f\"**Error:** {exc}\"))\n",
        "\n",
        "make_button.on_click(_make_attribute_coco)\n",
        "display(ui)\n",
        "\n",
        "# Populate dropdown initially\n",
        "_refresh_attribute_choices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Train/Validation/Test Splitter and Format Converter\n",
        "\n",
        "This section provides an interactive tool to (i) **split a COCO dataset** deterministically into Train/Validation/Test subsets and (ii) **materialise the dataset** in one of two formats:\n",
        "- **YOLO (Ultralytics v8/v11/v12)**: creates `images/{train,val,test}` and `labels/{train,val,test}` with YOLO-TXT labels.\n",
        "- **COCO-based**: creates `images/{train,val,test}` and `annotations/{train.json,val.json,test.json}`.\n",
        "\n",
        "You will frequently need reproducible splits and a dataset layout that matches your chosen training framework. This tool guarantees the same split for the same image filenames and offers a one-click conversion to either YOLO or COCO split structure.\n",
        "\n",
        "#### Usage\n",
        "1. Select the **Source** COCO file (e.g., `COCO.json` or `COCO_view_angle.json`).  \n",
        "2. Pick **Architecture** (YOLO or COCO-based).  \n",
        "3. Set the **Train/Val/Test** sliders; choose a **Freeze** policy if needed (to keep one split fixed)\n",
        "4. Check the **live readout** to confirm counts.  \n",
        "5. Click **Convert**. The output directory will be created with a name like:\n",
        "   - `YOLO_COCO` (for `COCO.json`) or `YOLO_COCO_view_angle` (for an attribute file), or\n",
        "   - `COCO_COCO` / `COCO_COCO_view_angle` for COCO-based splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PATHS ===\n",
        "COCO_SRC_DIR = OUTPUT_DIR                 # where COCO*.json files live\n",
        "DATASET_DIR  = OUTPUT_DIR.parent          # where new converted datasets will be created\n",
        "IMAGES_ROOT  = OUTPUT_DIR / \"images\"      # extracted images live here\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _load_json(p: Path) -> Dict[str, Any]:\n",
        "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def _save_json(obj: Dict[str, Any], p: Path) -> None:\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with p.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def _list_candidate_coco_files() -> List[Path]:\n",
        "    return sorted([p for p in COCO_SRC_DIR.glob(\"COCO*.json\") if p.is_file()])\n",
        "\n",
        "def _safe_name(s: str) -> str:\n",
        "    return \"\".join(ch if ch.isalnum() or ch in (\"_\", \"-\", \".\") else \"_\" for ch in s)\n",
        "\n",
        "def _link_or_copy(src: Path, dst: Path):\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        if dst.exists():\n",
        "            return\n",
        "        os.link(src, dst)\n",
        "    except Exception:\n",
        "        try:\n",
        "            if dst.exists():\n",
        "                return\n",
        "            os.symlink(src, dst)\n",
        "        except Exception:\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "def _split_ids_deterministic_3way(images: List[Dict[str, Any]], tr: float, va: float) -> Tuple[Set[int], Set[int], Set[int]]:\n",
        "    train_ids, val_ids, test_ids = set(), set(), set()\n",
        "    for im in images:\n",
        "        fname = str(im.get(\"file_name\", \"\"))\n",
        "        h = int(hashlib.md5(fname.encode(\"utf-8\")).hexdigest(), 16)\n",
        "        r = (h % 10_000_000) / 10_000_000.0\n",
        "        if r < tr:\n",
        "            train_ids.add(im[\"id\"])\n",
        "        elif r < tr + va:\n",
        "            val_ids.add(im[\"id\"])\n",
        "        else:\n",
        "            test_ids.add(im[\"id\"])\n",
        "    return train_ids, val_ids, test_ids\n",
        "\n",
        "# ---------- converters ----------\n",
        "def _coco_to_yolo(coco: Dict[str, Any], out_dir: Path, tr: float, va: float) -> Dict[str, Any]:\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    cats = sorted(coco.get(\"categories\", []), key=lambda c: c[\"id\"])\n",
        "    class_names = [c[\"name\"] for c in cats]\n",
        "    catid_to_idx = {c[\"id\"]: i for i, c in enumerate(cats)}\n",
        "\n",
        "    images = coco.get(\"images\", [])\n",
        "    id2img = {im[\"id\"]: im for im in images}\n",
        "    train_ids, val_ids, test_ids = _split_ids_deterministic_3way(images, tr, va)\n",
        "\n",
        "    for split in (\"train\", \"val\", \"test\"):\n",
        "        (out_dir / f\"images/{split}\").mkdir(parents=True, exist_ok=True)\n",
        "        (out_dir / f\"labels/{split}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    img_to_anns: Dict[int, List[Dict[str, Any]]] = {}\n",
        "    for ann in coco.get(\"annotations\", []):\n",
        "        img_to_anns.setdefault(ann[\"image_id\"], []).append(ann)\n",
        "\n",
        "    def _write_split(split_name: str, id_set: Set[int]):\n",
        "        for img_id in sorted(id_set):\n",
        "            im = id2img[img_id]\n",
        "            w, h = float(im[\"width\"]), float(im[\"height\"])\n",
        "            src_img = (IMAGES_ROOT / im[\"file_name\"]).resolve()\n",
        "            if not src_img.exists():\n",
        "                raise FileNotFoundError(f\"Image not found: {src_img}\")\n",
        "            dst_img = out_dir / f\"images/{split_name}/{_safe_name(Path(im['file_name']).name)}\"\n",
        "            _link_or_copy(src_img, dst_img)\n",
        "\n",
        "            anns = img_to_anns.get(img_id, [])\n",
        "            yolo_lines = []\n",
        "            for a in anns:\n",
        "                cls_idx = catid_to_idx[a[\"category_id\"]]\n",
        "                x, y, bw, bh = a[\"bbox\"]\n",
        "                xc = (x + bw / 2.0) / w\n",
        "                yc = (y + bh / 2.0) / h\n",
        "                nw = bw / w\n",
        "                nh = bh / h\n",
        "                yolo_lines.append(f\"{cls_idx} {xc:.6f} {yc:.6f} {nw:.6f} {nh:.6f}\")\n",
        "\n",
        "            lbl_path = out_dir / f\"labels/{split_name}/{dst_img.stem}.txt\"\n",
        "            with lbl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"\\n\".join(yolo_lines))\n",
        "\n",
        "    for s, ids in ((\"train\", train_ids), (\"val\", val_ids), (\"test\", test_ids)):\n",
        "        _write_split(s, ids)\n",
        "\n",
        "    data_yaml = {\n",
        "        \"path\": str(out_dir.resolve()),\n",
        "        \"train\": \"images/train\",\n",
        "        \"val\": \"images/val\",\n",
        "        \"test\": \"images/test\",\n",
        "        \"names\": class_names,\n",
        "    }\n",
        "    _save_json(data_yaml, out_dir / \"data.yaml.json\")\n",
        "\n",
        "    return {\n",
        "        \"classes\": class_names,\n",
        "        \"train\": len(train_ids),\n",
        "        \"val\": len(val_ids),\n",
        "        \"test\": len(test_ids),\n",
        "        \"format\": \"YOLO\",\n",
        "    }\n",
        "\n",
        "def _coco_to_coco_splits(coco: Dict[str, Any], out_dir: Path, tr: float, va: float) -> Dict[str, Any]:\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    images = coco.get(\"images\", [])\n",
        "    id2img = {im[\"id\"]: im for im in images}\n",
        "    train_ids, val_ids, test_ids = _split_ids_deterministic_3way(images, tr, va)\n",
        "\n",
        "    def _filtered(ids: Set[int]) -> Dict[str, Any]:\n",
        "        imgs = [id2img[i] for i in sorted(ids)]\n",
        "        img_set = {im[\"id\"] for im in imgs}\n",
        "        anns = [a for a in coco.get(\"annotations\", []) if a[\"image_id\"] in img_set]\n",
        "        return {\n",
        "            \"info\": coco.get(\"info\", {}),\n",
        "            \"licenses\": coco.get(\"licenses\", []),\n",
        "            \"images\": imgs,\n",
        "            \"annotations\": anns,\n",
        "            \"categories\": coco.get(\"categories\", []),\n",
        "        }\n",
        "\n",
        "    train_coco = _filtered(train_ids)\n",
        "    val_coco   = _filtered(val_ids)\n",
        "    test_coco  = _filtered(test_ids)\n",
        "\n",
        "    ann_dir = out_dir / \"annotations\"\n",
        "    _save_json(train_coco, ann_dir / \"train.json\")\n",
        "    _save_json(val_coco,   ann_dir / \"val.json\")\n",
        "    _save_json(test_coco,  ann_dir / \"test.json\")\n",
        "\n",
        "    for split, ids in ((\"train\", train_ids), (\"val\", val_ids), (\"test\", test_ids)):\n",
        "        split_img_dir = out_dir / f\"images/{split}\"\n",
        "        for img_id in ids:\n",
        "            im = id2img[img_id]\n",
        "            src = (IMAGES_ROOT / im[\"file_name\"]).resolve()\n",
        "            if not src.exists():\n",
        "                raise FileNotFoundError(f\"Image not found: {src}\")\n",
        "            dst = split_img_dir / _safe_name(Path(im[\"file_name\"]).name)\n",
        "            _link_or_copy(src, dst)\n",
        "\n",
        "    return {\n",
        "        \"classes\": [c[\"name\"] for c in sorted(coco[\"categories\"], key=lambda c: c[\"id\"])],\n",
        "        \"train\": len(train_ids),\n",
        "        \"val\": len(val_ids),\n",
        "        \"test\": len(test_ids),\n",
        "        \"format\": \"COCO\",\n",
        "    }\n",
        "\n",
        "# ---------- UI ----------\n",
        "ARCH_OPTS = [\n",
        "    \"YOLO (Ultralytics v8/v11/v12)\",\n",
        "    \"COCO-based (MMDetection, RF-DETR, RetinaNet, Faster R-CNN, EfficientDet)\"\n",
        "]\n",
        "\n",
        "_src_files = _list_candidate_coco_files()\n",
        "src_dropdown = widgets.Dropdown(\n",
        "    options=[str(p.name) for p in _src_files] if _src_files else [\"<no COCO files found>\"],\n",
        "    description=\"Source:\",\n",
        "    layout=widgets.Layout(width=\"420px\")\n",
        ")\n",
        "arch_dropdown = widgets.Dropdown(\n",
        "    options=ARCH_OPTS,\n",
        "    description=\"Architecture:\",\n",
        "    layout=widgets.Layout(width=\"420px\")\n",
        ")\n",
        "\n",
        "# Three sliders + \"Freeze\" selector\n",
        "train_slider = widgets.IntSlider(value=70, min=1, max=98, step=1, description=\"Train (%)\", continuous_update=True)\n",
        "val_slider   = widgets.IntSlider(value=15, min=1, max=98, step=1, description=\"Val (%)\",   continuous_update=True)\n",
        "test_slider  = widgets.IntSlider(value=15, min=1, max=98, step=1, description=\"Test (%)\",  continuous_update=True)\n",
        "\n",
        "hold_dropdown = widgets.Dropdown(\n",
        "    options=[\"Freeze Test\", \"Freeze Val\", \"Freeze Train\"],\n",
        "    value=\"Freeze Test\",\n",
        "    description=\"Freeze:\",\n",
        "    layout=widgets.Layout(width=\"200px\")\n",
        ")\n",
        "\n",
        "live_readout = widgets.HTML(\"\")\n",
        "convert_btn  = widgets.Button(description=\"Convert\", button_style=\"warning\", icon=\"exchange\", disabled=False)\n",
        "conv_out     = widgets.Output(layout=widgets.Layout(border=\"1px solid #ddd\", padding=\"8px\"))\n",
        "\n",
        "ui_box = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Convert COCO with Train/Val/Test Split for Object Detection</b>\"),\n",
        "    src_dropdown, arch_dropdown,\n",
        "    widgets.HBox([train_slider, val_slider, test_slider, hold_dropdown]),\n",
        "    live_readout,\n",
        "    convert_btn,\n",
        "    conv_out\n",
        "])\n",
        "display(ui_box)\n",
        "\n",
        "# ----- slider coupling -----\n",
        "_updating = False\n",
        "\n",
        "def _recompute(changed: str):\n",
        "    global _updating\n",
        "    if _updating:\n",
        "        return\n",
        "    _updating = True\n",
        "\n",
        "    tr, va, te = train_slider.value, val_slider.value, test_slider.value\n",
        "    hold = hold_dropdown.value\n",
        "\n",
        "    tr = max(1, min(98, tr))\n",
        "    va = max(1, min(98, va))\n",
        "    te = max(1, min(98, te))\n",
        "\n",
        "    if changed == \"train\":\n",
        "        if hold == \"Freeze Test\":\n",
        "            te = test_slider.value\n",
        "            va = max(1, 100 - tr - te)\n",
        "        elif hold == \"Freeze Val\":\n",
        "            va = val_slider.value\n",
        "            te = max(1, 100 - tr - va)\n",
        "        else:\n",
        "            tr = train_slider.value\n",
        "            va = max(1, 100 - tr - te)\n",
        "\n",
        "    elif changed == \"val\":\n",
        "        if hold == \"Freeze Test\":\n",
        "            te = test_slider.value\n",
        "            tr = max(1, 100 - va - te)\n",
        "        elif hold == \"Freeze Val\":\n",
        "            va = val_slider.value\n",
        "            te = max(1, 100 - tr - va)\n",
        "        else:\n",
        "            tr = train_slider.value\n",
        "            te = max(1, 100 - tr - va)\n",
        "\n",
        "    elif changed == \"test\":\n",
        "        if hold == \"Freeze Test\":\n",
        "            te = test_slider.value\n",
        "            va = max(1, 100 - tr - te)\n",
        "        elif hold == \"Freeze Val\":\n",
        "            va = val_slider.value\n",
        "            tr = max(1, 100 - va - te)\n",
        "        else:\n",
        "            tr = train_slider.value\n",
        "            va = max(1, 100 - tr - te)\n",
        "\n",
        "    total = tr + va + te\n",
        "    if total != 100:\n",
        "        delta = 100 - total\n",
        "        if changed == \"train\":\n",
        "            target = \"val\" if hold == \"Freeze Test\" else (\"test\" if hold == \"Freeze Val\" else \"val\")\n",
        "        elif changed == \"val\":\n",
        "            target = \"train\" if hold == \"Freeze Test\" else (\"test\" if hold == \"Freeze Train\" else \"train\")\n",
        "        else:\n",
        "            target = \"val\" if hold == \"Freeze Train\" else (\"train\" if hold == \"Freeze Val\" else \"val\")\n",
        "\n",
        "        if target == \"train\":\n",
        "            tr = max(1, min(98, tr + delta))\n",
        "        elif target == \"val\":\n",
        "            va = max(1, min(98, va + delta))\n",
        "        else:\n",
        "            te = max(1, min(98, te + delta))\n",
        "\n",
        "    train_slider.value, val_slider.value, test_slider.value = tr, va, te\n",
        "    _updating = False\n",
        "    _update_readout()\n",
        "\n",
        "def _current_coco():\n",
        "    candidates = {p.name: p for p in _list_candidate_coco_files()}\n",
        "    sel = src_dropdown.value\n",
        "    if sel not in candidates:\n",
        "        return None, None\n",
        "    p = candidates[sel]\n",
        "    return _load_json(p), p\n",
        "\n",
        "def _update_readout(*_):\n",
        "    coco, path = _current_coco()\n",
        "    if coco is None:\n",
        "        live_readout.value = \"No COCO selected.\"\n",
        "        convert_btn.disabled = True\n",
        "        return\n",
        "    tr, va, te = train_slider.value, val_slider.value, test_slider.value\n",
        "    images = coco.get(\"images\", [])\n",
        "    tr_ids, va_ids, te_ids = _split_ids_deterministic_3way(images, tr/100.0, va/100.0)\n",
        "    live_readout.value = (\n",
        "        f\"<b>Split:</b> Train {tr}% ({len(tr_ids)}) | Val {va}% ({len(va_ids)}) | \"\n",
        "        f\"Test {te}% ({len(te_ids)}) | Total: {len(images)} | Source: {path.name}\"\n",
        "    )\n",
        "    convert_btn.disabled = (tr + va + te != 100) or (tr < 1 or va < 1 or te < 1)\n",
        "\n",
        "def _on_train_change(change):\n",
        "    if change[\"name\"] == \"value\":\n",
        "        _recompute(\"train\")\n",
        "\n",
        "def _on_val_change(change):\n",
        "    if change[\"name\"] == \"value\":\n",
        "        _recompute(\"val\")\n",
        "\n",
        "def _on_test_change(change):\n",
        "    if change[\"name\"] == \"value\":\n",
        "        _recompute(\"test\")\n",
        "\n",
        "def _handle_convert(_):\n",
        "    with conv_out:\n",
        "        conv_out.clear_output()\n",
        "        coco, src_path = _current_coco()\n",
        "        if coco is None:\n",
        "            display(Markdown(\"No valid COCO source selected.\"))\n",
        "            return\n",
        "        tr, va, te = train_slider.value, val_slider.value, test_slider.value\n",
        "        if tr + va + te != 100 or min(tr, va, te) < 1:\n",
        "            display(Markdown(\"Invalid split: ensure Train+Val+Test=100 and each ≥ 1%.\"))\n",
        "            return\n",
        "\n",
        "        arch = arch_dropdown.value\n",
        "        arch_name = _safe_name(arch.split()[0])\n",
        "        out_dir = DATASET_DIR / f\"{arch_name}_{src_path.stem}\"\n",
        "\n",
        "        try:\n",
        "            if arch.startswith(\"YOLO\"):\n",
        "                result = _coco_to_yolo(coco, out_dir, tr/100.0, va/100.0)\n",
        "                display(Markdown(f\"YOLO dataset ready at <b>{out_dir}</b>\"))\n",
        "            else:\n",
        "                result = _coco_to_coco_splits(coco, out_dir, tr/100.0, va/100.0)\n",
        "                display(Markdown(f\"COCO split ready at <b>{out_dir}</b>\"))\n",
        "            print(f\"Images — train: {result['train']}, val: {result['val']}, test: {result['test']}\")\n",
        "            print(f\"Classes: {', '.join(result['classes'])}\")\n",
        "        except Exception as exc:\n",
        "            display(Markdown(f\"Error: {exc}\"))\n",
        "\n",
        "# wire events\n",
        "train_slider.observe(_on_train_change, names=\"value\")\n",
        "val_slider.observe(_on_val_change, names=\"value\")\n",
        "test_slider.observe(_on_test_change, names=\"value\")\n",
        "src_dropdown.observe(lambda _ : _update_readout(), names=\"value\")\n",
        "hold_dropdown.observe(lambda _ : _update_readout(), names=\"value\")\n",
        "\n",
        "convert_btn.on_click(_handle_convert)\n",
        "\n",
        "# initial normalize & preview\n",
        "_recompute(\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37786ad3",
      "metadata": {},
      "source": [
        "# What’s Next\n",
        "\n",
        "After completing the conversion step above, your dataset has been properly split and formatted for training.  \n",
        "\n",
        "If you selected **YOLO**, your next step is to **create the `data.yaml` file** that defines the dataset paths and class names. This file is required by YOLO to begin training.  \n",
        "\n",
        "If you selected a **COCO-based architecture** (such as RF-DETR, RetinaNet, Faster R-CNN, or EfficientDet), your next step is to **register the dataset inside your chosen framework** by linking the generated `train.json`, `val.json`, and `test.json` files with their image directories.  \n",
        "\n",
        "In both cases, ensure your splits are balanced and your class definitions are correct before proceeding to the model training phase.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ARI3129",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
